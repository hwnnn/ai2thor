"""
ë¡œì»¬ LLM (Ollama gpt-oss) ì‚¬ìš© ì˜ˆì œ
ì™„ì „ ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”, ì˜¤í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥
"""

import json
import os
from multi_agent_system import (
    FunctionDatabase, 
    LLMTaskPlanner, 
    MultiAgentOrchestrator
)


def check_ollama_running():
    """Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸"""
    import subprocess
    try:
        result = subprocess.run(
            ["curl", "-s", "http://localhost:11434/api/tags"],
            capture_output=True,
            timeout=2
        )
        if result.returncode == 0:
            print("âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return True
        else:
            print("âŒ Ollama ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        print(f"âŒ Ollama ì„œë²„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


def check_gpt_oss_installed():
    """gpt-oss ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    import subprocess
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if "gpt-oss" in result.stdout:
            print("âœ… gpt-oss ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return True
        else:
            print("âŒ gpt-oss ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜ ë°©ë²•: ollama pull gpt-oss:20b")
            return False
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False


def example_local_llm_sequential():
    """ì˜ˆì œ: ë¡œì»¬ LLMìœ¼ë¡œ ìˆœì°¨ì  ì‘ì—… ìˆ˜í–‰"""
    print("\n" + "="*80)
    print("ì˜ˆì œ: ë¡œì»¬ LLM (gpt-oss) - ìˆœì°¨ì  ì‘ì—…")
    print("="*80)
    
    # Ollama í™•ì¸
    if not check_ollama_running():
        print("\nâŒ Ollama ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”:")
        print("   macOS: brew install ollama && ollama serve")
        print("   Linux: curl -fsSL https://ollama.com/install.sh | sh && ollama serve")
        return
    
    if not check_gpt_oss_installed():
        print("\nâŒ gpt-oss ëª¨ë¸ì„ ë¨¼ì € ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("   ollama pull gpt-oss:20b")
        return
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¡œì»¬ LLM ì‚¬ìš©)
    function_db = FunctionDatabase()
    llm_planner = LLMTaskPlanner(function_db, use_local=True)  # ë¡œì»¬ ì‚¬ìš©!
    orchestrator = MultiAgentOrchestrator(function_db, llm_planner)
    
    command = "scene 1ì—ì„œ í† ë§ˆí† ë¥¼ ì°ê³ , ë¶ˆì„ ì¼œê³  ë‹«ê³ , ëƒ‰ì¥ê³ ì— í† ë§ˆí† ë¥¼ ë„£ì–´."
    
    try:
        print(f"\nğŸ’¬ ëª…ë ¹ì–´: {command}")
        print("â³ ë¡œì»¬ LLM ë¶„ì„ ì¤‘...\n")
        
        result = orchestrator.execute_natural_language_command(
            command=command,
            scene="FloorPlan1",
            max_agents=5
        )
        
        # ê²°ê³¼ ì €ì¥
        output_file = "results/example_local_llm_sequential.json"
        os.makedirs("results", exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"âœ… ì‚¬ìš©ëœ agent ìˆ˜: {result['num_agents']}")
        print(f"âœ… ë¶„ì„: {result['analysis']}")
        print("\nğŸ’¡ ë¡œì»¬ LLM ì‚¬ìš©ìœ¼ë¡œ API ë¹„ìš© 0ì›!")
        
        return result
        
    finally:
        orchestrator.shutdown_all_agents()


def example_local_llm_parallel():
    """ì˜ˆì œ: ë¡œì»¬ LLMìœ¼ë¡œ ë³‘ë ¬ ì‘ì—… ìˆ˜í–‰"""
    print("\n" + "="*80)
    print("ì˜ˆì œ: ë¡œì»¬ LLM (gpt-oss) - ë³‘ë ¬ ì‘ì—…")
    print("="*80)
    
    if not check_ollama_running() or not check_gpt_oss_installed():
        return
    
    function_db = FunctionDatabase()
    llm_planner = LLMTaskPlanner(function_db, use_local=True)
    orchestrator = MultiAgentOrchestrator(function_db, llm_planner)
    
    command = """
    agent 1ì€ ì£¼ë°©ì—ì„œ ì‚¬ê³¼ë¥¼ ì°¾ì•„ì„œ ê°€ì ¸ì˜¤ê³ ,
    agent 2ëŠ” ê±°ì‹¤ì—ì„œ TVë¥¼ ì¼œê³ ,
    agent 3ëŠ” ì¹¨ì‹¤ì„ íƒìƒ‰í•´ì„œ ëª¨ë“  ì„œëì„ ì—´ì–´.
    """
    
    try:
        print(f"\nğŸ’¬ ëª…ë ¹ì–´: {command}")
        print("â³ ë¡œì»¬ LLM ë¶„ì„ ì¤‘...\n")
        
        result = orchestrator.execute_natural_language_command(
            command=command,
            scene="FloorPlan1",
            max_agents=5
        )
        
        output_file = "results/example_local_llm_parallel.json"
        os.makedirs("results", exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"âœ… ì‚¬ìš©ëœ agent ìˆ˜: {result['num_agents']}")
        print(f"âœ… ë¶„ì„: {result['analysis']}")
        print("\nğŸ’¡ ë¡œì»¬ LLM ì‚¬ìš©ìœ¼ë¡œ API ë¹„ìš© 0ì›!")
        
        return result
        
    finally:
        orchestrator.shutdown_all_agents()


def compare_local_vs_cloud():
    """ë¡œì»¬ LLM vs í´ë¼ìš°ë“œ API ë¹„êµ"""
    print("\n" + "="*80)
    print("ë¡œì»¬ LLM vs í´ë¼ìš°ë“œ API ë¹„êµ")
    print("="*80)
    
    comparison = """
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      í•­ëª©          â”‚  ë¡œì»¬ (gpt-oss)    â”‚  í´ë¼ìš°ë“œ (GPT-4)  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ ë¹„ìš©               â”‚ ë¬´ë£Œ (0ì›)         â”‚ ìœ ë£Œ (ì¢…ëŸ‰ì œ)      â”‚
    â”‚ API í‚¤ í•„ìš”        â”‚ ë¶ˆí•„ìš”             â”‚ í•„ìš”               â”‚
    â”‚ ì¸í„°ë„· ì—°ê²°        â”‚ ë¶ˆí•„ìš”             â”‚ í•„ìˆ˜               â”‚
    â”‚ í”„ë¼ì´ë²„ì‹œ         â”‚ ì™„ì „ ë³´ì¥          â”‚ ë°ì´í„° ì „ì†¡        â”‚
    â”‚ ì†ë„               â”‚ í•˜ë“œì›¨ì–´ ì˜ì¡´      â”‚ ë¹ ë¦„               â”‚
    â”‚ í’ˆì§ˆ               â”‚ ì¢‹ìŒ               â”‚ ìµœê³                â”‚
    â”‚ ì„¤ì¹˜               â”‚ í•„ìš” (~12GB)       â”‚ ë¶ˆí•„ìš”             â”‚
    â”‚ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­    â”‚ 16GB RAM           â”‚ ì—†ìŒ               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ğŸ’¡ ì¶”ì²œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
    
    ğŸ“¦ ë¡œì»¬ LLM (gpt-oss):
    - ê°œë°œ ë° í…ŒìŠ¤íŠ¸
    - í”„ë¡œí† íƒ€ì´í•‘
    - í”„ë¼ì´ë²„ì‹œê°€ ì¤‘ìš”í•œ ê²½ìš°
    - ë¹„ìš© ì ˆê°ì´ í•„ìš”í•œ ê²½ìš°
    - ì¸í„°ë„· ì—°ê²°ì´ ë¶ˆì•ˆì •í•œ í™˜ê²½
    
    â˜ï¸ í´ë¼ìš°ë“œ API (GPT-4):
    - í”„ë¡œë•ì…˜ í™˜ê²½
    - ìµœê³  í’ˆì§ˆì´ í•„ìš”í•œ ê²½ìš°
    - ë¡œì»¬ ë¦¬ì†ŒìŠ¤ê°€ ë¶€ì¡±í•œ ê²½ìš°
    - ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ê²½ìš°
    """
    
    print(comparison)


def setup_instructions():
    """Ollama ë° gpt-oss ì„¤ì¹˜ ì•ˆë‚´"""
    print("\n" + "="*80)
    print("Ollama ë° gpt-oss ì„¤ì¹˜ ë°©ë²•")
    print("="*80)
    
    instructions = """
    
    ğŸ macOS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Ollama ì„¤ì¹˜
    brew install ollama
    
    # 2. Ollama ì„œë²„ ì‹œì‘
    ollama serve
    
    # 3. ìƒˆ í„°ë¯¸ë„ì—ì„œ gpt-oss ë‹¤ìš´ë¡œë“œ
    ollama pull gpt-oss:20b
    
    
    ğŸ§ Linux:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Ollama ì„¤ì¹˜
    curl -fsSL https://ollama.com/install.sh | sh
    
    # 2. Ollama ì„œë²„ ì‹œì‘
    ollama serve
    
    # 3. ìƒˆ í„°ë¯¸ë„ì—ì„œ gpt-oss ë‹¤ìš´ë¡œë“œ
    ollama pull gpt-oss:20b
    
    
    ğŸ³ Docker (ëª¨ë“  OS):
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1. Ollama ì»¨í…Œì´ë„ˆ ì‹¤í–‰
    docker run -d -v ollama:/root/.ollama -p 11434:11434 \\
        --name ollama ollama/ollama
    
    # 2. gpt-oss ë‹¤ìš´ë¡œë“œ
    docker exec -it ollama ollama pull gpt-oss:20b
    
    # 3. ëª¨ë¸ ì‹¤í–‰
    docker exec -it ollama ollama run gpt-oss:20b
    
    
    âœ… ì„¤ì¹˜ í™•ì¸:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # Ollama ì„œë²„ í™•ì¸
    curl http://localhost:11434/api/tags
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
    ollama list
    
    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    ollama run gpt-oss:20b "ì•ˆë…•í•˜ì„¸ìš”!"
    
    
    ğŸ“¦ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - RAM: 16GB ì´ìƒ (20b ëª¨ë¸)
    - ë””ìŠ¤í¬: 15GB ì´ìƒ ì—¬ìœ  ê³µê°„
    - CPU: ë‹¤ì¤‘ ì½”ì–´ (GPU ì„ íƒì‚¬í•­)
    
    
    ğŸš€ ì‚¬ìš© ì‹œì‘:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì˜ˆì œ ì‹¤í–‰
    python example_local_llm.py 1    # ìˆœì°¨ì  ì‘ì—…
    python example_local_llm.py 2    # ë³‘ë ¬ ì‘ì—…
    python example_local_llm.py compare  # ë¹„êµí‘œ ë³´ê¸°
    """
    
    print(instructions)
    print("\nğŸ“š ìì„¸í•œ ì •ë³´: GPT_OSS_LOCAL_SETUP.md ì°¸ê³ ")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "1":
            example_local_llm_sequential()
        elif command == "2":
            example_local_llm_parallel()
        elif command == "compare":
            compare_local_vs_cloud()
        elif command == "setup":
            setup_instructions()
        else:
            print("ì‚¬ìš©ë²•:")
            print("  python example_local_llm.py 1        # ìˆœì°¨ì  ì‘ì—… ì˜ˆì œ")
            print("  python example_local_llm.py 2        # ë³‘ë ¬ ì‘ì—… ì˜ˆì œ")
            print("  python example_local_llm.py compare  # ë¡œì»¬ vs í´ë¼ìš°ë“œ ë¹„êµ")
            print("  python example_local_llm.py setup    # ì„¤ì¹˜ ì•ˆë‚´")
    else:
        print("\n" + "="*80)
        print("ë¡œì»¬ LLM (Ollama gpt-oss) ì˜ˆì œ")
        print("="*80)
        print("\nğŸ’¡ ì™„ì „ ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”, ì˜¤í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥!\n")
        
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹:")
        print("  python example_local_llm.py 1        # ìˆœì°¨ì  ì‘ì—… ì˜ˆì œ")
        print("  python example_local_llm.py 2        # ë³‘ë ¬ ì‘ì—… ì˜ˆì œ")
        print("  python example_local_llm.py compare  # ë¡œì»¬ vs í´ë¼ìš°ë“œ ë¹„êµ")
        print("  python example_local_llm.py setup    # ì„¤ì¹˜ ì•ˆë‚´")
        
        print("\n" + "="*80)
        setup_instructions()


if __name__ == "__main__":
    main()
